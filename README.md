# ğŸ§ ğŸ¤Ÿ AI-Powered ASL Translator

**Translate American Sign Language into text using AI â€” real-time via webcam or through uploaded images. Train and compare ML models easily with a beautiful web interface.**

> Bridging the gap between silence and speech using Computer Vision and Machine Learning.

**ğŸ”— Live Demo:** [asl-translator-x4wf.onrender.com](https://asl-translator-x4wf.onrender.com)  
**ğŸ“ GitHub Repo:** [github.com/miyasajid19/ASL-Translator](https://github.com/miyasajid19/ASL-Translator.git)

![image](https://github.com/user-attachments/assets/2b68dbc6-43f2-4baf-a7b2-9ad75ab7c6cc)
  

---

## âœ¨ Features

- **ğŸ“¹ Real-Time Translation:** Translate ASL signs via webcam using MediaPipe + Scikit-learn.
- **ğŸ–¼ï¸ Image-Based Detection:** Upload multiple images and recognize ASL signs with your trained model.
- **ğŸ§  Train Custom Models:** Choose from ML classifiers like Random Forest, SVM, Logistic Regression, etc.
- **ğŸ“Š Compare Algorithms:** Evaluate multiple classifiers on the same dataset with interactive visualizations.
- **ğŸ“ˆ Insightful Charts:** Built-in Plotly and Chart.js integration for data insights.
- **ğŸ§© Modular Design:** Easy to extend or modify any component â€” backend, models, or UI.

---

## ğŸ§° Tech Stack

**Backend**
- Python 3.8+
- Flask + Flask-SocketIO
- OpenCV
- MediaPipe (Google)
- Scikit-learn
- Pandas, NumPy

**Frontend**
- HTML, CSS (Bootstrap 5)
- JavaScript (jQuery, vanilla)
- Plotly.js & Chart.js
- Socket.IO Client

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- `pip`
- Git
- Webcam (for RTP)

### Installation

```bash
git clone https://github.com/miyasajid19/ASL-Translator.git
cd ASL-Translator

# Create and activate virtual environment
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Secret Key Setup

```bash
# macOS/Linux
export SECRET_KEY="your_random_secure_key"
# Windows CMD
set SECRET_KEY=your_random_secure_key
```

### Run the App

```bash
python main.py
# Visit http://localhost:5000
```

---

## ğŸ—‚ï¸ Folder Structure

```
ASL-Translator/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ static/
â”‚   â””â”€â”€ files/
â”‚       â”œâ”€â”€ datasets/    # Input datasets (.pkl)
â”‚       â””â”€â”€ models/      # Trained models (.pkl)
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ real_time_processing.html
â”‚   â”œâ”€â”€ image_processing.html
â”‚   â”œâ”€â”€ train.html
â”‚   â””â”€â”€ compare_models.html
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Use

### ğŸ”´ Real-Time ASL (`/RTP`)
- Choose a trained model.
- Start webcam.
- Perform ASL signs â€” see predictions live and track recognized sequence.

### ğŸ–¼ï¸ Image Upload (`/ImageProcessing`)
- Choose a model.
- Upload multiple images.
- See individual predictions + combined sequence output.

### ğŸ“š Train Model (`/train`)
- Select dataset + algorithm.
- Enter a unique model name.
- View accuracy, training time, plots â€” model is saved.

### ğŸ“Š Compare Models (`/compare`)
- Choose a dataset + multiple algorithms.
- Get detailed metrics (accuracy, precision, F1-score).
- Compare results in charts & tables.

---

## ğŸ“¦ Data Format

### Datasets (`.pkl`)
```python
{
  'data': [list of 42 floats],  # Flattened (x,y) hand landmarks
  'label': ['A', 'B', 'Hello', ...]
}
```

### Models (`.pkl`)
```python
{
  'model': sklearn classifier,
  'accuracy': float,
  'algorithm_key': str,
  ...
}
```

---

## âš™ï¸ Configuration Highlights

In `main.py`:

```python
SECRET_KEY = os.environ.get("SECRET_KEY")
MODEL_DIR = './static/files/models/'
DATASET_DIR = './static/files/datasets/'
EXPECTED_LANDMARK_FEATURES = 42
NO_HAND_TIMEOUT = 2.0
```

---

## ğŸ›£ Roadmap

- [x] Webcam-based static ASL translation
- [x] Image batch upload
- [x] Model training & saving
- [x] Comparison of classifiers
- [ ] Dynamic sign recognition (word-level or sentence-level)
- [ ] Dataset creation module
- [ ] Docker deployment support
- [ ] Speech output integration (TTS)

---

## ğŸ¤ Contributing

```bash
# Fork -> Clone -> Create Branch -> Commit -> PR
git checkout -b feature/your-feature-name
```

Please follow PEP8 and document major changes. Open issues for discussions.

---

## âš–ï¸ License

Licensed under the **MIT License**.  
See [`LICENSE`](./LICENSE) for more details.

---

## ğŸ™ Credits

- [MediaPipe](https://mediapipe.dev/)
- [Scikit-learn](https://scikit-learn.org/)
- [Flask](https://flask.palletsprojects.com/)
- [OpenCV](https://opencv.org/)
- [Bootstrap](https://getbootstrap.com/)
- [Plotly](https://plotly.com/)
- [Chart.js](https://www.chartjs.org/)

---

Made with â¤ï¸ by **Sajid Miya**

[ğŸ”— Live App](https://asl-translator-x4wf.onrender.com) â€¢ [ğŸ“ GitHub](https://github.com/miyasajid19/ASL-Translator.git)
